min.dens <- apply(y.mat[c(ii,jj),],2,min)
this.no  <- sum(min.dens)*dd
out.mat[ii,jj] <- this.no
out.mat[jj,ii] <- this.no
}
out.mat
}
NO_emp1=no.cts.fn(sp = dat$lab,yy = dat$MAT)[1,2]
no.cts.fn <- function(sp,yy)
{
# Do density estimation for each species:
# The density functions must have matching values of x.
# Go well outside the actual data at ends, to get to
# zero density.
lowx <- min(yy)-2*sd(yy)
topx <- max(yy)+2*sd(yy)
# Define the x values for which the density functions
# will be calculated. These must match. Go well
# outside the actual data at top end, to get to zero density.
# Lowest value at 0, highest at topx.
# Let nn = number of x values.
nn   <- 500
# Find dd, the distance between spikes:
dd <- (topx-lowx)/(nn-1)
x.vect <- seq(lowx,topx,length.out=nn)
y.mat  <- matrix(NA,no.spp,nn)
spnames   <- unique(sp)
no.spp=length(spnames)
for (ii in 1:no.spp)
{
this.sp <- spnames[ii]
this.y  <- yy[sp==this.sp]
this.y  <- this.y[!is.na(this.y)]  # Remove missing values
this.n  <- length(this.y)          # Number of usable observations
# Need at least two points to select a bandwidth for density est.
if (length(this.y)>1)      # Density estimation is possible
{
this.dens <- density(this.y, n=nn, from=lowx, to=topx)[[2]]
this.dens=this.dens#/sum(this.dens)
y.mat[ii,] <- this.dens
}
}
# Find niche overlap for each pair of species.
out.mat  <- matrix(1,no.spp,no.spp)
for (ii in 1:(no.spp-1)) for (jj in (ii+1):no.spp)
{
min.dens <- apply(y.mat[c(ii,jj),],2,min)
this.no  <- sum(min.dens)*dd
out.mat[ii,jj] <- this.no
out.mat[jj,ii] <- this.no
}
out.mat
}
dat$MAT
dat$lab
nr1
nr2
NO_emp1=no.cts.fn(sp = dat$lab,yy = dat$MAT)[1,2]
no.cts.fn <- function(sp,yy)
{
# Do density estimation for each species:
# The density functions must have matching values of x.
# Go well outside the actual data at ends, to get to
# zero density.
lowx <- min(yy)-2*sd(yy)
topx <- max(yy)+2*sd(yy)
# Define the x values for which the density functions
# will be calculated. These must match. Go well
# outside the actual data at top end, to get to zero density.
# Lowest value at 0, highest at topx.
# Let nn = number of x values.
nn   <- 500
# Find dd, the distance between spikes:
dd <- (topx-lowx)/(nn-1)
no.spp=length(spnames)
x.vect <- seq(lowx,topx,length.out=nn)
y.mat  <- matrix(NA,no.spp,nn)
spnames   <- unique(sp)
for (ii in 1:no.spp)
{
this.sp <- spnames[ii]
this.y  <- yy[sp==this.sp]
this.y  <- this.y[!is.na(this.y)]  # Remove missing values
this.n  <- length(this.y)          # Number of usable observations
# Need at least two points to select a bandwidth for density est.
if (length(this.y)>1)      # Density estimation is possible
{
this.dens <- density(this.y, n=nn, from=lowx, to=topx)[[2]]
this.dens=this.dens#/sum(this.dens)
y.mat[ii,] <- this.dens
}
}
# Find niche overlap for each pair of species.
out.mat  <- matrix(1,no.spp,no.spp)
for (ii in 1:(no.spp-1)) for (jj in (ii+1):no.spp)
{
min.dens <- apply(y.mat[c(ii,jj),],2,min)
this.no  <- sum(min.dens)*dd
out.mat[ii,jj] <- this.no
out.mat[jj,ii] <- this.no
}
out.mat
}
NO_emp1=no.cts.fn(sp = dat$lab,yy = dat$MAT)[1,2]
no.cts.fn <- function(sp,yy)
{
spnames   <- unique(sp)
no.spp=length(spnames)
# Do density estimation for each species:
# The density functions must have matching values of x.
# Go well outside the actual data at ends, to get to
# zero density.
lowx <- min(yy)-2*sd(yy)
topx <- max(yy)+2*sd(yy)
# Define the x values for which the density functions
# will be calculated. These must match. Go well
# outside the actual data at top end, to get to zero density.
# Lowest value at 0, highest at topx.
# Let nn = number of x values.
nn   <- 500
# Find dd, the distance between spikes:
dd <- (topx-lowx)/(nn-1)
x.vect <- seq(lowx,topx,length.out=nn)
y.mat  <- matrix(NA,no.spp,nn)
for (ii in 1:no.spp)
{
this.sp <- spnames[ii]
this.y  <- yy[sp==this.sp]
this.y  <- this.y[!is.na(this.y)]  # Remove missing values
this.n  <- length(this.y)          # Number of usable observations
# Need at least two points to select a bandwidth for density est.
if (length(this.y)>1)      # Density estimation is possible
{
this.dens <- density(this.y, n=nn, from=lowx, to=topx)[[2]]
this.dens=this.dens#/sum(this.dens)
y.mat[ii,] <- this.dens
}
}
# Find niche overlap for each pair of species.
out.mat  <- matrix(1,no.spp,no.spp)
for (ii in 1:(no.spp-1)) for (jj in (ii+1):no.spp)
{
min.dens <- apply(y.mat[c(ii,jj),],2,min)
this.no  <- sum(min.dens)*dd
out.mat[ii,jj] <- this.no
out.mat[jj,ii] <- this.no
}
out.mat
}
NO_emp1=no.cts.fn(sp = dat$lab,yy = dat$MAT)[1,2]
NO_emp2=no.cts.fn(sp = dat$lab,yy = dat$MAP)[1,2]
NO_comb=mean(NO_emp1,NO_emp2)
NO_emp1
#randomly swapping mountain regions and recalculating the similarities in climate
null=matrix(NA,nrow = 1000,ncol=2)
for(i in 1:1000){
sam=sample(dat$lab)
null[i,1]=no.cts.fn(sp = sam,yy = dat$MAT)[1,2]
null[i,2]=no.cts.fn(sp = sam,yy = dat$MAT)[1,2]
}
set.seed(123)
null=matrix(NA,nrow = 1000,ncol=2)
for(i in 1:1000){
sam=sample(dat$lab)
null[i,1]=no.cts.fn(sp = sam,yy = dat$MAT)[1,2]
null[i,2]=no.cts.fn(sp = sam,yy = dat$MAT)[1,2]
}
#Then taking the mean following eqn. 5 in Geange et al 2011
null_mean=apply(null,1,mean)
hist(null_mean)
abline(v=NO_comb)
P_clim=1-length(which(null_mean>NO_comb))/1000
P_clim
nr1
#loading list with habitat types for each mountain regions, retreved from Jung et al. (2020)
load("Data files/hab_list.RData")
hab_dat=data.frame(clims,cols=pal1,freq_beta=NA,freq_alpha=NA,freq_both=NA)
hab_dat=data.frame(clims,cols=pal1,freq_beta=NA,freq_alpha=NA,freq_both=NA)
clims=c(400,600,800,100,200,300,500)
hab_dat=data.frame(clims,cols=pal1,freq_beta=NA,freq_alpha=NA,freq_both=NA)
hab_dat=data.frame(clims,freq_beta=NA,freq_alpha=NA,freq_both=NA)
for(i in 1:nrow(pie_dat)){
#if(i <nrow(pie_dat)){
pos=which(names(ex_un1)==clims[i])
hab_dat$freq_beta[i]=ex_un1[pos]
pos=which(names(ex_un2)==clims[i])
hab_dat$freq_alpha[i]=ex_un2[pos]
pos=which(names(ex_un3)==clims[i])
if(length(pos)>0){hab_dat$freq_both[i]=ex_un3[pos]} else hab_dat$freq_both[i]=0
}
for(i in 1:nrow(hab_dat)){
#if(i <nrow(pie_dat)){
pos=which(names(ex_un1)==clims[i])
hab_dat$freq_beta[i]=ex_un1[pos]
pos=which(names(ex_un2)==clims[i])
hab_dat$freq_alpha[i]=ex_un2[pos]
pos=which(names(ex_un3)==clims[i])
if(length(pos)>0){hab_dat$freq_both[i]=ex_un3[pos]} else hab_dat$freq_both[i]=0
}
hab_dat
ex_nr1=unlist(extract(KG1,mount1[nr1,]))
ex_nr1=hab_list[[nr1]]
ex_nr1=hab_list[nr1]
ex_nr1
ex_nr1=hab_list[nr1]
ex_nr2=hab_list[nr2]
ex_nr3=hab_list[nr3]
ex_nr1=ex_nr1[which(ex_nr1<1400)]
ex_nr2=ex_nr2[which(ex_nr2<1400)]
ex_nr3=ex_nr3[which(ex_nr3<1400)]
ex_nr1
ex_nr1=ex_nr1[which(ex_nr1<1400)]
ex_nr1=unlist(hab_list[nr1])
ex_nr2=unlist(hab_list[nr2])
ex_nr3=unlist(hab_list[nr3])
ex_nr1=ex_nr1[which(ex_nr1<1400)]
ex_nr2=ex_nr2[which(ex_nr2<1400)]
ex_nr3=ex_nr3[which(ex_nr3<1400)]
ex_nr1=round(ex_nr1,-2)
ex_nr2=round(ex_nr2,-2)
ex_nr3=round(ex_nr3,-2)
ex_un=sort(table(c(ex_nr1,ex_nr2)),decreasing = T)
ex_un1=sort(table(ex_nr1),decreasing = T)
ex_un2=sort(table(ex_nr2),decreasing = T)
ex_un3=sort(table(ex_nr3),decreasing = T)
ex_un3
hab_dat=data.frame(clims,freq_beta=NA,freq_alpha=NA,freq_both=NA)
for(i in 1:nrow(hab_dat)){
#if(i <nrow(pie_dat)){
pos=which(names(ex_un1)==clims[i])
hab_dat$freq_beta[i]=ex_un1[pos]
pos=which(names(ex_un2)==clims[i])
hab_dat$freq_alpha[i]=ex_un2[pos]
pos=which(names(ex_un3)==clims[i])
if(length(pos)>0){hab_dat$freq_both[i]=ex_un3[pos]} else hab_dat$freq_both[i]=0
}
hab_dat
ex_un1
hab_dat=data.frame(clims,freq_beta=ex_un1,freq_alpha=ex_un2)
hab_dat
hab_dat=data.frame(clims,freq_beta=c(ex_un1),freq_alpha=ex_un2)
hab_dat
ex_un1=c(sort(table(ex_nr1),decreasing = T))
ex_un2=c(sort(table(ex_nr2),decreasing = T))
ex_un3=c(sort(table(ex_nr3),decreasing = T))
hab_dat=data.frame(clims,freq_beta=c(ex_un1),freq_alpha=ex_un2)
hab_dat
hab_dat=apply(hab_dat[,-1],2,function(x){x/sum(x)})
hab_dat
hab_dat=data.frame(clims,freq_beta=c(ex_un1),freq_alpha=ex_un2)
hab_dat[,-1]
hab_dat=data.frame(freq_beta=c(ex_un1),freq_alpha=ex_un2)
hab_dat
apply(hab_dat,2,function(x){x/sum(x)})
apply(hab_dat,1,function(x){x/sum(x)})
hab_dat=apply(hab_dat,2,function(x){x/sum(x)})
hab_dat
round(hab_dat,5)
hab_dat=data.frame(freq_beta=c(ex_un1),freq_alpha=ex_un2)
hab_dat=apply(hab_dat,2,function(x){x/sum(x)})
hab_dat=t(hab_dat)
obs_hab=no.cat.fn_JS(hab_dat)[1,2]
hab_dat=data.frame(freq_beta=c(ex_un1),freq_alpha=ex_un2)
hab_dat=apply(hab_dat,2,function(x){x/sum(x)})
hab_dat=t(hab_dat)
obs_hab=no.cat.fn_JS(hab_dat)[1,2]
no.cat.fn_JS <- function(props.mat)
{
no_spp=nrow(props.mat)
out.mat <- matrix(1,no.spp,no.spp)
for (ii in 1:(no.spp-1)) for (jj in (ii+1):no.spp)
{
small.mat <- props.mat[c(ii,jj),]
mins <- apply(small.mat,2,min)
out.mat[ii,jj] <- sum(mins)
out.mat[jj,ii] <- sum(mins)
}
out.mat
}
obs_hab=no.cat.fn_JS(hab_dat)[1,2]
no.cat.fn_JS <- function(props.mat)
{
no.spp=nrow(props.mat)
out.mat <- matrix(1,no.spp,no.spp)
for (ii in 1:(no.spp-1)) for (jj in (ii+1):no.spp)
{
small.mat <- props.mat[c(ii,jj),]
mins <- apply(small.mat,2,min)
out.mat[ii,jj] <- sum(mins)
out.mat[jj,ii] <- sum(mins)
}
out.mat
}
obs_hab=no.cat.fn_JS(hab_dat)[1,2]
obs_hab
all
all
all
all=c(nr1,nr2)
all
comb
nr1_null=sample(comb,length(nr1))
comb=c(nr1,nr2)
nr1_null
nr1_null=sample(comb,length(nr1))
nr2_null=all[which(!all %in% nr1_null)]
nr1_null
hab_dat=data.frame(freq_beta=c(ex_un1),freq_alpha=ex_un2)
hab_dat=apply(hab_dat,2,function(x){x/sum(x)})
hab_dat=t(hab_dat)
no.cat.fn_JS(dat2)
no.cat.fn_JS(hab_dat)
comb=c(nr1,nr2)
null=rep(NA,1000)
for(y in 1:1000){
nr1_null=sample(comb,length(nr1))
nr2_null=all[which(!all %in% nr1_null)]
ex_nr1=unlist(clim_list[nr1_null])
ex_nr2=unlist(clim_list[nr2_null])
ex_nr1=ex_nr1[which(ex_nr1<1400)]
ex_nr2=ex_nr2[which(ex_nr2<1400)]
ex_nr1=round(ex_nr1,-2)
ex_nr2=round(ex_nr2,-2)
ex_un1=c(sort(table(ex_nr1),decreasing = T))
ex_un2=c(sort(table(ex_nr2),decreasing = T))
hab_dat=data.frame(freq_beta=c(ex_un1),freq_alpha=ex_un2)
hab_dat=apply(hab_dat,2,function(x){x/sum(x)})
hab_dat=t(hab_dat)
null[y]=no.cat.fn_JS(hab_dat)[1,2]
print(y)
}
null[y]=no.cat.fn_JS(hab_dat)[1,2]
nr1_null=sample(comb,length(nr1))
nr2_null=all[which(!all %in% nr1_null)]
ex_nr1=unlist(clim_list[nr1_null])
ex_nr2=unlist(clim_list[nr2_null])
nr1_null=sample(comb,length(nr1))
nr2_null=all[which(!all %in% nr1_null)]
ex_nr1=unlist(hab_dat[nr1_null])
ex_nr2=unlist(hab_dat[nr2_null])
ex_nr1=ex_nr1[which(ex_nr1<1400)]
ex_nr2=ex_nr2[which(ex_nr2<1400)]
ex_nr1=round(ex_nr1,-2)
ex_nr2=round(ex_nr2,-2)
ex_un1=c(sort(table(ex_nr1),decreasing = T))
ex_un2=c(sort(table(ex_nr2),decreasing = T))
hab_dat=data.frame(freq_beta=c(ex_un1),freq_alpha=ex_un2)
hab_dat=apply(hab_dat,2,function(x){x/sum(x)})
hab_dat=t(hab_dat)
hab_dat
hab_dat
nr1_null=sample(comb,length(nr1))
nr2_null=all[which(!all %in% nr1_null)]
ex_nr1=unlist(hab_list[nr1_null])
ex_nr2=unlist(hab_list[nr2_null])
ex_nr1=ex_nr1[which(ex_nr1<1400)]
ex_nr2=ex_nr2[which(ex_nr2<1400)]
ex_nr1=round(ex_nr1,-2)
ex_nr2=round(ex_nr2,-2)
ex_un1=c(sort(table(ex_nr1),decreasing = T))
ex_un2=c(sort(table(ex_nr2),decreasing = T))
hab_dat=data.frame(freq_beta=c(ex_un1),freq_alpha=ex_un2)
hab_dat=apply(hab_dat,2,function(x){x/sum(x)})
hab_dat=t(hab_dat)
hab_dat
null[y]=no.cat.fn_JS(hab_dat)[1,2]
null[y]
load("Data files/hab_list.RData")
clims=c(400,600,800,100,200,300,500)
hab_dat=data.frame(freq_beta=c(ex_un1),freq_alpha=ex_un2)
hab_dat=apply(hab_dat,2,function(x){x/sum(x)})
hab_dat=t(hab_dat)
obs_hab=no.cat.fn_JS(hab_dat)[1,2]
comb=c(nr1,nr2)
null=rep(NA,1000)
for(y in 1:1000){
nr1_null=sample(comb,length(nr1))
nr2_null=all[which(!all %in% nr1_null)]
ex_nr1=unlist(hab_list[nr1_null])
ex_nr2=unlist(hab_list[nr2_null])
ex_nr1=ex_nr1[which(ex_nr1<1400)]
ex_nr2=ex_nr2[which(ex_nr2<1400)]
ex_nr1=round(ex_nr1,-2)
ex_nr2=round(ex_nr2,-2)
ex_un1=c(sort(table(ex_nr1),decreasing = T))
ex_un2=c(sort(table(ex_nr2),decreasing = T))
hab_dat=data.frame(freq_beta=c(ex_un1),freq_alpha=ex_un2)
hab_dat=apply(hab_dat,2,function(x){x/sum(x)})
hab_dat=t(hab_dat)
null[y]=no.cat.fn_JS(hab_dat)[1,2]
print(y)
}
nr1_null=sample(comb,length(nr1))
nr2_null=all[which(!all %in% nr1_null)]
ex_nr1=unlist(hab_list[nr1_null])
ex_nr2=unlist(hab_list[nr2_null])
ex_nr1=ex_nr1[which(ex_nr1<1400)]
ex_nr2=ex_nr2[which(ex_nr2<1400)]
ex_nr1=round(ex_nr1,-2)
ex_nr2=round(ex_nr2,-2)
ex_un1=c(sort(table(ex_nr1),decreasing = T))
ex_un2=c(sort(table(ex_nr2),decreasing = T))
hab_dat=data.frame(freq_beta=c(ex_un1),freq_alpha=ex_un2)
hab_dat=apply(hab_dat,2,function(x){x/sum(x)})
hab_dat=t(hab_dat)
for(y in 1:1000){
nr1_null=sample(comb,length(nr1))
nr2_null=all[which(!all %in% nr1_null)]
ex_nr1=unlist(hab_list[nr1_null])
ex_nr2=unlist(hab_list[nr2_null])
ex_nr1=ex_nr1[which(ex_nr1<1400)]
ex_nr2=ex_nr2[which(ex_nr2<1400)]
ex_nr1=round(ex_nr1,-2)
ex_nr2=round(ex_nr2,-2)
ex_un1=c(sort(table(ex_nr1),decreasing = T))
ex_un2=c(sort(table(ex_nr2),decreasing = T))
hab_dat=data.frame(freq_beta=c(ex_un1),freq_alpha=ex_un2)
hab_dat=apply(hab_dat,2,function(x){x/sum(x)})
hab_dat=t(hab_dat)
null[y]=no.cat.fn_JS(hab_dat)[1,2]
print(y)
}
set.seed(123)
comb=c(nr1,nr2)
null=rep(NA,1000)
for(y in 1:1000){
nr1_null=sample(comb,length(nr1))
nr2_null=all[which(!all %in% nr1_null)]
ex_nr1=unlist(hab_list[nr1_null])
ex_nr2=unlist(hab_list[nr2_null])
ex_nr1=ex_nr1[which(ex_nr1<1400)]
ex_nr2=ex_nr2[which(ex_nr2<1400)]
ex_nr1=round(ex_nr1,-2)
ex_nr2=round(ex_nr2,-2)
ex_un1=c(sort(table(ex_nr1),decreasing = T))
ex_un2=c(sort(table(ex_nr2),decreasing = T))
hab_dat=data.frame(freq_beta=c(ex_un1),freq_alpha=ex_un2)
hab_dat=apply(hab_dat,2,function(x){x/sum(x)})
hab_dat=t(hab_dat)
null[y]=no.cat.fn_JS(hab_dat)[1,2]
print(y)
}
hist(null)
abline(v=NO_hab[1,2])
abline(v=obs_hab[1,2])
obs_hab
abline(v=obs_hab)
hab_dat
#assembling frequency distribution of habitat types for the 10% of mountain regions with highest local species richness and turnover
hab_dat=data.frame(freq_beta=c(ex_un1),freq_alpha=ex_un2)
hab_dat=apply(hab_dat,2,function(x){x/sum(x)})
hab_dat=t(hab_dat)
#calculating the similarity in habitat compositions following eqn. 2 applicable for categorical data (Geange et al. 2011)
obs_hab=no.cat.fn_JS(hab_dat)[1,2]
obs_hab
hab_dat
ex_nr1=unlist(hab_list[nr1])
ex_nr2=unlist(hab_list[nr2])
ex_nr1=ex_nr1[which(ex_nr1<1400)]
ex_nr2=ex_nr2[which(ex_nr2<1400)]
ex_nr1=round(ex_nr1,-2)
ex_nr2=round(ex_nr2,-2)
ex_un1=c(sort(table(ex_nr1),decreasing = T))
ex_un2=c(sort(table(ex_nr2),decreasing = T))
hab_dat=data.frame(freq_beta=c(ex_un1),freq_alpha=ex_un2)
hab_dat=apply(hab_dat,2,function(x){x/sum(x)})
hab_dat=t(hab_dat)
#calculating the similarity in habitat compositions following eqn. 2 applicable for categorical data (Geange et al. 2011)
obs_hab=no.cat.fn_JS(hab_dat)[1,2]
se
obs_hab
abline(v=obs_hab)
P_hab=1-length(which(null>obs_hab))/1000
P_hab
nr3
rep(1,length(nr3))
dat
dat=data.frame(geo=rep(0,nrow(loc)),lab="none")
dat$lab[c(nr1)]="beta"
dat$lab[c(nr2)]="alpha"
dat$geo[c(nr1)]=1
dat=rbind(subset(dat,dat$lab=="alpha"),subset(dat,dat$lab=="beta"))
dat=rbind(dat,data.frame(geo=rep(1,length(nr3)),lab="alpha"))
dat=rbind(dat,data.frame(geo=rep(0,length(nr3)),lab="beta"))
dat
sam=sample(dat$lab)
sam
#randomly swapping mountain regions and recalculating the overlap
set.seed(123)
null=rep(NA,nrow = 1000)
for(i in 1:1000){
sam=sample(dat$lab)
null[i]=no.bin.fn(sp = sam,yy = dat$geo)[1,2]
}
#calculating the null model permutation p-value (P)
P_geo=1-length(which(null>obs_overlap))/1000
P_geo
############################################## calculating overlap probability
setwd("/Users/jespersonne/Documents/GitHub/Local-richness-and-turnover")
#Functions used for calculating similarities between two groups with respect to binary, categorical and continuous data types
#All functions were retrived from Geange, Shane W., et al. "A unified analysis of niche overlap incorporating data of different types." Methods in Ecology and Evolution 2.2 (2011): 175-184.
source("R functions.R")
10/5
5/10
getwd()
dd=read.csv("Data files/std_species turnover_50gridcells.csv")
head(dd)
dd[1:10,1:10]
dd=read.csv("Data files/std_species turnover_50gridcells.csv")[,-1]
dd[1:10,1:10]
dd-1
write.csv(dd-1,"Data files/std_species turnover_50gridcells.csv")
